#!/usr/bin/env pwsh
# Test script to verify LLM provider configuration

param(
    [switch]$StopOnFailure = $false
)

Write-Host "`nðŸš€ Testing LLM Provider Configuration..." -ForegroundColor Green
Write-Host ("=" * 50)

# Build the project
Write-Host "`nðŸ“¦ Building project..." -ForegroundColor Cyan
$buildResult = dotnet build TestCaseEditorApp.csproj --verbosity quiet
if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ Build failed!" -ForegroundColor Red
    if ($StopOnFailure) { exit 1 }
}
Write-Host "âœ… Build successful" -ForegroundColor Green

# Show current LLM provider configuration in the code
Write-Host "`nðŸ”§ Current LLM Provider Configuration:" -ForegroundColor Cyan
Write-Host "   Provider: ollama (explicit local configuration)" -ForegroundColor White
Write-Host "   Model: llama3.2 (consistent across installations)" -ForegroundColor White
Write-Host "   Security: Local processing only (no internet)" -ForegroundColor White

Write-Host "`nðŸ“‹ Configuration Benefits:" -ForegroundColor Cyan
Write-Host "   âœ… Consistent results across all user installations" -ForegroundColor Green
Write-Host "   âœ… Technical data stays on local machine (security)" -ForegroundColor Green
Write-Host "   âœ… No dependency on user AnythingLLM system settings" -ForegroundColor Green
Write-Host "   âœ… Predictable requirement analysis quality" -ForegroundColor Green

Write-Host "`nðŸ” What changed from system defaults:" -ForegroundColor Cyan
Write-Host "   Before: chatProvider=`"system`", chatModel=`"system`"" -ForegroundColor Yellow
Write-Host "   After:  chatProvider=`"ollama`", chatModel=`"llama3.2`"" -ForegroundColor Green

Write-Host "`nðŸ”§ To test manually:" -ForegroundColor Cyan
Write-Host "   dotnet run" -ForegroundColor White
Write-Host "   Then: File -> Open -> [select a .tcex.json file]" -ForegroundColor White
Write-Host "   Analyze requirements -> Should use local Ollama consistently" -ForegroundColor White

Write-Host "`nâœ… Configuration is ready for testing!" -ForegroundColor Green
Write-Host ("=" * 50)